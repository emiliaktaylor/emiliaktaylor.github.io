<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://leozhangml.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://leozhangml.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-26T17:47:54+00:00</updated><id>https://leozhangml.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">scaling up bayesian nonparametric manifold learning</title><link href="https://leozhangml.github.io/blog/2023/manifold_vi/" rel="alternate" type="text/html" title="scaling up bayesian nonparametric manifold learning"/><published>2023-11-26T16:40:16+00:00</published><updated>2023-11-26T16:40:16+00:00</updated><id>https://leozhangml.github.io/blog/2023/manifold_vi</id><content type="html" xml:base="https://leozhangml.github.io/blog/2023/manifold_vi/"><![CDATA[<p>This is an excert from my master’s thesis, for which the code can be found <a href="https://github.com/leozhangML/manifold_vi">here</a>.</p> <p>The manifold hypothesis states that naturally generated, high-dimensional data, such as images or neural activity, should be expected to lie around some low-dimensional manifold. This has been justified by the fact that natural data is generated under physical constraints which limit its intrinsic complexity. The success of machine learning methods, such as neural networks, on high-dimensional data can thus be explained by their ability to exploit this latent, low-dimensional structure.</p> <p>The field of manifold learning (Bengio et al. [2014]) then is concerned with leveraging the intuition behind the manifold hypothesis to design machine learning algorithms, which can represent these geometric structures. For example, the algorithms: Isomap (Tenenbaum et al. [2000]) and Umap (McInnes et al. [2018]), apply geometric concepts to provide dimensionality reduction for data visualisation purposes.</p> <p>Another area of manifold learning is density estimation. While density estimation is an established problem within statistics, being useful in downstream clustering or prediction tasks, the manifold hypothesis suggests a natural framework for estimation. This has seen particular interest from research in deep generative models (Tomczak [2022]) due the fact that such models aim to learn the distribution of our data to generate new, interesting samples. By accounting for the geometric structure of data, such models should be able to learn better data distributions for improved performance. Brown et al. [2023] presents evidence in this direction, showing that inductive biases within deep generative models, which account for the manifold hypothesis, can help improve model performance. Also, Horvat and Pfister [2023] adapts normalising flows to learn distributions supported on low-dimensional submanifolds.</p> <p>In this dissertation, however, we are interested in the Bayesian nonparametric approach to density estimation. The standard way this is done is through Dirichlet process mixtures - this specifies our density by an infinite mixture model, with a Dirichlet process prior placed on the mixture’s parameters. Bayesian inference then provides an estimate of the true density. While such models have long been studied in the literature (Neal [2000]),they mainly assume that the distribution of our data is supported on $\mathbb{R}^D$ instead of possessing some low-dimensional structure. Hence, these approaches have been shown to struggle in recovering the density of manifold-distributed data as they are unable to capture the curvature and (low) dimensionality of such structures - see Mukhopadhyay et al.[2020].</p> <p>The recent paper Berenfeld et al. [2022] overcomes these limitation through introducing a new family of Dirichlet process mixtures, for which they show very good empirical performance. Theoretical guarantees are also provided to show that such mixtures are able to converge to the “true” distribution of the data. Along with the interpretability of such methods compared to black-box deep generative models, this provides a compelling solution to density estimation under the manifold hypothesis. The main drawback, however,is that inference - through MCMC sampling - is computationally expensive and fails to scale to datasets with dimension greater than two. This prevents the use of such methods beyond toy examples.</p> <p>This motivates the main contributions of this dissertation. We present a variational inference algorithm to provide approximate Bayesian inference for Berenfeld et al. [2022]. This allows us to scale such methods to higher-dimensional datasets, for which we empirically observe only a slight drop in accuracy. We also attempt to give theoretical guarantees for the convergence rate of an oracle variational approximation to the true parameters, in order to provide justification for the use of variational inference. We note that while we were unable to finish the proof before the dissertation deadline, our proof only requires one final technical step.</p> <h3 id="references">References:</h3> <p>Bengio, Y., Courville, A. and Vincent, P., 2013. Representation learning: A review and new perspectives. <em>IEEE transactions on pattern analysis and machine intelligence, 35</em>(8), pp.1798-1828.</p> <p>Berenfeld, C., Rosa, P. and Rousseau, J., 2022. Estimating a density near an unknown manifold: a Bayesian nonparametric approach. <em>arXiv preprint arXiv:2205.15717</em>.</p> <p>Brown, B.C., Caterini, A.L., Ross, B.L., Cresswell, J.C. and Loaiza-Ganem, G., 2022. Verifying the union of manifolds hypothesis for image data. In <em>The Eleventh International Conference on Learning Representations</em>.</p> <p>Horvat, C. and Pfister, J.P., 2023. Density estimation on low-dimensional manifolds: an inflation-deflation approach. <em>J. Mach. Learn. Res., 24</em>, pp.61-1.</p> <p>McInnes, L., Healy, J. and Melville, J., 2018. Umap: Uniform manifold approximation and projection for dimension reduction. <em>arXiv preprint arXiv:1802.03426</em>.</p> <p>Mukhopadhyay, M., Li, D. and Dunson, D.B., 2020. Estimating densities with non-linear support by using Fisher–Gaussian kernels. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology, 82</em>(5), pp.1249-1271.</p> <p>Neal, R.M., 2000. Markov chain sampling methods for Dirichlet process mixture models. <em>Journal of computational and graphical statistics, 9</em>(2), pp.249-265.</p> <p>Tenenbaum, J.B., Silva, V.D. and Langford, J.C., 2000. A global geometric framework for nonlinear dimensionality reduction. <em>science</em>, 290(5500), pp.2319-2323.</p> <p>Tomczak, J.M., 2022. Deep Generative Modeling. <em>Springer</em>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[description of my master's thesis]]></summary></entry><entry><title type="html">a post with formatting and links</title><link href="https://leozhangml.github.io/blog/2015/formatting-and-links/" rel="alternate" type="text/html" title="a post with formatting and links"/><published>2015-03-15T16:40:16+00:00</published><updated>2015-03-15T16:40:16+00:00</updated><id>https://leozhangml.github.io/blog/2015/formatting-and-links</id><content type="html" xml:base="https://leozhangml.github.io/blog/2015/formatting-and-links/"><![CDATA[<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h4 id="hipster-list">Hipster list</h4> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> <p>Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90’s yr typewriter selfies letterpress cardigan vegan.</p> <hr/> <p>Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.</p> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <p>Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[march & april, looking forward to summer]]></summary></entry></feed>